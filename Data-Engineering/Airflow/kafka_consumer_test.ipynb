{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559d4112-0696-49ee-a61e-c261cb6f9e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8431a744b9bd48bf962672358d3a0912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57740635-c4d7-4aa4-95f2-26f908906fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Importing kafka libraries\n",
    "#from kafka import KafkaProducer\n",
    "#from kafka import KafkaConsumer\n",
    "import csv\n",
    "import json\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.master(\"local\").appName(\"ETL\").getOrCreate()\n",
    "spark = SparkSession.builder.appName('LinkitTest').getOrCreate()\n",
    "from py4j.java_gateway import java_import\n",
    "java_import(spark._sc._jvm, \"org.apache.spark.sql.api.python.*\")\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StringType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107f37f4-3822-4dbe-a2a0-07ee04019ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Kafka configuration\n",
    "bootstrap_servers = ['10.21.0.208:30753']\n",
    "topic = 'csv_data_topic'\n",
    "\n",
    "# Define the folder path\n",
    "delta_path = \"s3a://truepoc-bkt-raw/kafka/\"\n",
    "\n",
    "# Kafka SASL configurations\n",
    "kafka_security_protocol = \"SASL_PLAINTEXT\"\n",
    "kafka_sasl_mechanism = \"PLAIN\"\n",
    "kafka_sasl_username = \"user1\"  # Replace with your SASL username\n",
    "kafka_sasl_password = \"user1\"  # Replace with your SASL password\n",
    "\n",
    "df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", bootstrap_servers)\n",
    "    .option(\"subscribe\", topic)\n",
    "    .option(\"kafka.security.protocol\", kafka_security_protocol)\n",
    "    .option(\"kafka.sasl.mechanism\", kafka_sasl_mechanism)\n",
    "    .option(\"kafka.sasl.jaas.config\",\n",
    "            f\"kafka.username={kafka_sasl_username};kafka.password={kafka_sasl_password};\")\n",
    "    .option(\"startingOffsets\", \"earliest\") # You can change the starting offset as needed\n",
    "    .option(\"includeHeaders\", \"true\") \n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5537c19-e38c-4096-9002-49015cc78822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: string, value: string, headers: array<struct<key:string,value:binary>>]"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"headers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "177b1b1d-497e-40a8-98b6-b0c578f4785f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = df.writeStream.format(\"console\").start()\n",
    "import time\n",
    "time.sleep(10) # sleep 10 seconds\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61da2a08-d0b6-4a44-9639-2a270cf73259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "[STREAM_FAILED] Query [id = d80ba6d1-81fe-487b-a762-9147730830a8, runId = 0fe3c3aa-6095-4f07-aa8a-56db6be552c4] terminated with exception: Failed to construct kafka consumer\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/mapr/spark/spark-3.4.0/python/lib/pyspark.zip/pyspark/sql/streaming/query.py\", line 201, in awaitTermination\n",
      "    return self._jsq.awaitTermination()\n",
      "  File \"/opt/mapr/spark/spark-3.4.0/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/mapr/spark/spark-3.4.0/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = d80ba6d1-81fe-487b-a762-9147730830a8, runId = 0fe3c3aa-6095-4f07-aa8a-56db6be552c4] terminated with exception: Failed to construct kafka consumer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the value column from Kafka to a string\n",
    "df = df.withColumn(\"value\", df[\"value\"].cast(StringType()))\n",
    "\n",
    "# Parse the JSON value if your messages are in JSON format\n",
    "# Modify this part according to your message structure\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING)\").select(from_json(\"value\", schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Display the messages from Kafka topic in the console\n",
    "query = (\n",
    "    parsed_df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Wait for the streaming query to finish\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b0723b-8d80-4dbb-bbc9-d6a5ecb447e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "'StreamingQuery' object has no attribute 'close'\n",
      "Traceback (most recent call last):\n",
      "AttributeError: 'StreamingQuery' object has no attribute 'close'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5eed8db-4ff3-4149-beec-f5c1c2fa566a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "'write' can not be called on streaming Dataset/DataFrame.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/mapr/spark/spark-3.4.0/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 510, in write\n",
      "    return DataFrameWriter(self)\n",
      "  File \"/opt/mapr/spark/spark-3.4.0/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 945, in __init__\n",
      "    self._jwrite = df._jdf.write()\n",
      "  File \"/opt/mapr/spark/spark-3.4.0/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/mapr/spark/spark-3.4.0/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 175, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: 'write' can not be called on streaming Dataset/DataFrame.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame as a Delta table in the S3 bucket ---Might takes some time---\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a9ab62-8568-4bd6-ba05-4225ac319a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
